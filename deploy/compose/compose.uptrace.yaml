# ==============================================================================
# Pressograph - Uptrace Observability Stack
# ==============================================================================
# Compose Spec 2025 - Podman Compose compatible
# No version field required per Compose Specification
#
# Provides OpenTelemetry APM platform with advanced tracing capabilities
#
# Components:
# - Uptrace: OpenTelemetry APM and observability platform
# - ClickHouse: Backend storage for traces, metrics, and logs
# - PostgreSQL: Metadata storage for Uptrace
#
# Usage:
#   task uptrace:start  # Start the stack
#   task uptrace:stop   # Stop the stack
#   task uptrace:logs   # View logs
#
# Access:
#   - Uptrace UI: https://dev-uptrace.infra4.dev
#   - ClickHouse: localhost:9000 (internal only)
#   - PostgreSQL: localhost:5433 (internal only)
#
# Environment Variables Required:
#   UPTRACE_PROJECT_ID=1
#   UPTRACE_PROJECT_TOKEN=<generate-with-uuidgen>
#   UPTRACE_ADMIN_EMAIL=admin@pressograph.local
#   UPTRACE_ADMIN_PASSWORD=<secure-password>

name: pressograph-uptrace

services:
  # ----------------------------------------------------------------------------
  # ClickHouse - Backend Storage for Uptrace
  # ----------------------------------------------------------------------------
  clickhouse:
    image: clickhouse/clickhouse-server:25.3.5
    container_name: pressograph-uptrace-clickhouse
    hostname: uptrace-clickhouse
    restart: unless-stopped
    user: "101:101"  # Non-root user (clickhouse:clickhouse)
    environment:
      # ClickHouse configuration
      - CLICKHOUSE_DB=uptrace
      - CLICKHOUSE_USER=uptrace
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD:-uptracepassword}
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
    volumes:
      - clickhouse-data:/var/lib/clickhouse
      - ./uptrace/clickhouse-config.xml:/etc/clickhouse-server/config.d/logging.xml:ro
    expose:
      - "9000"   # Native protocol (internal use)
      - "8123"   # HTTP interface (internal use)
    # NO public port mapping
    networks:
      uptrace-net:
    healthcheck:
      test: ["CMD", "wget", "-q", "-O-", "http://0.0.0.0:8123/ping"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 2G
          cpus: '1.0'
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - SYS_NICE
    labels:
      io.podman.compose.project: pressograph-uptrace
      com.pressograph.service: clickhouse

  # ----------------------------------------------------------------------------
  # PostgreSQL - Metadata Storage for Uptrace
  # ----------------------------------------------------------------------------
  postgres-uptrace:
    image: postgres:18-trixie
    container_name: pressograph-uptrace-postgres
    hostname: uptrace-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_DB=uptrace
      - POSTGRES_USER=uptrace
      - POSTGRES_PASSWORD=${UPTRACE_POSTGRES_PASSWORD:-uptracepassword}
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - postgres-uptrace-data:/var/lib/postgresql/data
    expose:
      - "5432"
    # NO public port mapping
    networks:
      uptrace-net:
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U uptrace -d uptrace"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - DAC_OVERRIDE
      - FOWNER
      - SETGID
      - SETUID
    labels:
      io.podman.compose.project: pressograph-uptrace
      com.pressograph.service: postgres-uptrace

  # ----------------------------------------------------------------------------
  # Valkey (Redis-compatible) - Task Queue + Cache Backend
  # ----------------------------------------------------------------------------
  valkey:
    image: docker.io/valkey/valkey:9-trixie
    container_name: pressograph-uptrace-valkey
    hostname: uptrace-valkey
    restart: unless-stopped
    command: valkey-server /etc/valkey/valkey.conf
    volumes:
      - valkey-data:/data
      - ./redis/valkey.conf:/etc/valkey/valkey.conf:ro
    expose:
      - "6379"
    networks:
      uptrace-net:
    healthcheck:
      test: ["CMD", "valkey-cli", "-h", "127.0.0.1", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '1.0'
        reservations:
          memory: 256M
          cpus: '0.25'
    security_opt:
      - no-new-privileges:true
    cap_drop:
      - ALL
    cap_add:
      - SETGID
      - SETUID
    labels:
      io.podman.compose.project: pressograph-uptrace
      com.pressograph.service: valkey

  # ----------------------------------------------------------------------------
  # Uptrace - OpenTelemetry APM Platform
  # ----------------------------------------------------------------------------
  uptrace:
    image: uptrace/uptrace:2.0.1
    container_name: pressograph-uptrace
    hostname: uptrace
    restart: unless-stopped
    volumes:
      - ./uptrace/uptrace.yml:/etc/uptrace/config.yml:ro
    # Note: Configuration values are in config.yml
    # Override secrets via environment variables if needed
    environment:
      - CLICKHOUSE_PASSWORD=${CLICKHOUSE_PASSWORD:-uptracepassword}
      - POSTGRES_PASSWORD=${UPTRACE_POSTGRES_PASSWORD:-uptracepassword}
    expose:
      - "4317"   # OTLP gRPC (internal)
      - "80"     # HTTP UI + OTLP HTTP (internal)
    # NO public port mapping - accessed via Traefik
    networks:
      uptrace-net:
      traefik-public:
    restart: unless-stopped
    depends_on:
      clickhouse:
        condition: service_healthy
      postgres-uptrace:
        condition: service_healthy
      valkey:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "-O-", "http://0.0.0.0:80/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '2.0'
        reservations:
          memory: 512M
          cpus: '0.5'
    security_opt:
      - no-new-privileges:true
    labels:
      # Podman Compose metadata
      io.podman.compose.project: pressograph-uptrace
      com.pressograph.service: uptrace

      # Traefik configuration
      traefik.enable: "true"
      traefik.docker.network: traefik-public

      # HTTPS Router for Web UI (Certificate from Cloudflare Origin CA)
      traefik.http.routers.uptrace-dev.rule: Host(`dev-uptrace.infra4.dev`)
      traefik.http.routers.uptrace-dev.entrypoints: https
      traefik.http.routers.uptrace-dev.tls: "true"
      traefik.http.routers.uptrace-dev.tls.domains[0].main: infra4.dev
      traefik.http.routers.uptrace-dev.tls.domains[0].sans: "*.infra4.dev"
      traefik.http.routers.uptrace-dev.middlewares: web-development@file
      traefik.http.routers.uptrace-dev.service: uptrace-dev

      # Service configuration (Web UI port - internal container port)
      traefik.http.services.uptrace-dev.loadbalancer.server.port: "80"

# ==============================================================================
# Volumes
# ==============================================================================
volumes:
  clickhouse-data:
    name: pressograph-uptrace-clickhouse-data
    driver: local
  postgres-uptrace-data:
    name: pressograph-uptrace-postgres-data
    driver: local
  valkey-data:
    name: pressograph-uptrace-valkey-data
    driver: local

# ==============================================================================
# Networks
# ==============================================================================
networks:
  uptrace-net:
    name: pressograph-uptrace-network
    driver: bridge
    internal: true  # No external access - isolated backend
    ipam:
      driver: default
      config:
        - subnet: 10.89.30.0/24
          gateway: 10.89.30.1

  traefik-public:
    external: true
    name: traefik-public
