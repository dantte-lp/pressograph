groups:
  - name: pressograph-availability
    interval: 1m
    rules:
      # Service availability alerts
      - alert: ServiceDown
        expr: up == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Service {{ $labels.job }} is down"
          description: "{{ $labels.job }} service has been down for more than 2 minutes."

      - alert: VictoriaMetricsDown
        expr: up{job="victoria-metrics"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "VictoriaMetrics is down"
          description: "VictoriaMetrics has been down for more than 1 minute. Metrics collection is stopped."

      - alert: GrafanaDown
        expr: up{job="grafana"} == 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Grafana is down"
          description: "Grafana has been down for more than 5 minutes."

  - name: pressograph-performance
    interval: 1m
    rules:
      # High request rate
      - alert: HighRequestRate
        expr: rate(http_requests_total[5m]) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High request rate detected"
          description: "Request rate is {{ $value }} requests/sec for more than 5 minutes."

      # High error rate
      - alert: HighErrorRate
        expr: rate(api_errors_total[5m]) > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors/sec for more than 5 minutes."

      # Slow requests (P95 latency)
      - alert: SlowRequests
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Slow requests detected"
          description: "P95 request latency is {{ $value }}s for more than 5 minutes."

  - name: pressograph-resources
    interval: 1m
    rules:
      # High memory usage
      - alert: HighMemoryUsage
        expr: process_resident_memory_bytes / 1024 / 1024 > 1024
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage on {{ $labels.job }}"
          description: "Memory usage is {{ $value }}MB for more than 5 minutes."

      # Database connection pool exhaustion
      - alert: DatabaseConnectionPoolExhausted
        expr: db_connections_active / db_connections_max > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Database connection pool near exhaustion"
          description: "Using {{ $value | humanizePercentage }} of database connection pool."

  # PostgreSQL Database Alerts
  - name: pressograph-database
    interval: 1m
    rules:
      # Too many database connections
      - alert: PostgreSQLTooManyConnections
        expr: sum(pg_stat_database_numbackends) by (instance) > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "PostgreSQL has too many connections"
          description: "PostgreSQL instance {{ $labels.instance }} has {{ $value }} active connections (threshold: 80)"

      # Dead tuples accumulating
      - alert: PostgreSQLDeadTuples
        expr: pg_stat_user_tables_n_dead_tup > 10000
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "PostgreSQL has too many dead tuples"
          description: "Table {{ $labels.relname }} has {{ $value }} dead tuples. Run VACUUM."

      # Slow queries
      - alert: PostgreSQLSlowQueries
        expr: rate(pg_stat_activity_max_tx_duration[5m]) > 60
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "PostgreSQL has slow queries"
          description: "Slow queries detected on {{ $labels.instance }}, max duration {{ $value }}s"

      # Database disk usage
      - alert: PostgreSQLDiskUsageHigh
        expr: pg_database_size_bytes / (1024*1024*1024) > 50
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "PostgreSQL database size is growing"
          description: "Database {{ $labels.datname }} is {{ $value }}GB"

      # Replication lag (if using replication)
      - alert: PostgreSQLReplicationLag
        expr: pg_replication_lag > 30
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "PostgreSQL replication lag detected"
          description: "Replication lag is {{ $value }}s on {{ $labels.instance }}"

  # Valkey/Redis Cache Alerts
  - name: pressograph-cache
    interval: 1m
    rules:
      # Low cache hit rate
      - alert: ValkeyCacheHitRateLow
        expr: rate(redis_keyspace_hits_total[5m]) / (rate(redis_keyspace_hits_total[5m]) + rate(redis_keyspace_misses_total[5m])) < 0.7
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: "Valkey cache hit rate is low"
          description: "Cache hit rate is {{ $value | humanizePercentage }} (threshold: 70%)"

      # High memory usage
      - alert: ValkeyMemoryHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Valkey memory usage is critical"
          description: "Memory usage is {{ $value | humanizePercentage }} on {{ $labels.instance }}"

      # Too many connected clients
      - alert: ValkeyTooManyClients
        expr: redis_connected_clients > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Valkey has too many clients"
          description: "{{ $value }} clients connected to {{ $labels.instance }}"

      # Rejected connections
      - alert: ValkeyRejectedConnections
        expr: rate(redis_rejected_connections_total[5m]) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "Valkey is rejecting connections"
          description: "{{ $value }} connections/sec rejected on {{ $labels.instance }}"

      # Evicted keys (memory pressure)
      - alert: ValkeyEvictingKeys
        expr: rate(redis_evicted_keys_total[5m]) > 10
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "Valkey is evicting keys due to memory pressure"
          description: "{{ $value }} keys/sec being evicted on {{ $labels.instance }}"

  - name: pressograph-storage
    interval: 5m
    rules:
      # VictoriaMetrics disk usage
      - alert: VictoriaMetricsHighDiskUsage
        expr: vm_free_disk_space_bytes / vm_data_size_bytes < 0.2
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "VictoriaMetrics disk usage is high"
          description: "Less than 20% disk space available for VictoriaMetrics data."

  - name: pressograph-business
    interval: 1m
    rules:
      # Low cache hit ratio
      - alert: LowCacheHitRatio
        expr: rate(cache_hits_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m])) < 0.5
        for: 10m
        labels:
          severity: info
        annotations:
          summary: "Low cache hit ratio"
          description: "Cache hit ratio is {{ $value | humanizePercentage }} for more than 10 minutes."

# Recording rules for common queries
  - name: pressograph-recording
    interval: 1m
    rules:
      # Pre-calculate request rate
      - record: job:http_requests:rate5m
        expr: rate(http_requests_total[5m])

      # Pre-calculate error rate
      - record: job:api_errors:rate5m
        expr: rate(api_errors_total[5m])

      # Pre-calculate P95 latency
      - record: job:http_request_duration:p95
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))

      # Pre-calculate cache hit ratio
      - record: job:cache_hit_ratio:rate5m
        expr: rate(cache_hits_total[5m]) / (rate(cache_hits_total[5m]) + rate(cache_misses_total[5m]))
